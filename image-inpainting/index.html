<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">

  <meta name="description" content="Beyond CNNs: Encoded Context for Image Inpainting with LSTMs and Pixel CNNs">
  <meta name="keywords"
    content="Image Inpainting, GAN, Pixel CNN, LSTM, EfficientNet, ResNet, VGG, Autoencoder, Convolutional Neural Network, Generative Adversarial Network, Wasserstein GAN, Context Encoder">
  <meta name="author" content="Taneem Ullah Jan">
  <link rel="canonical" href="https://taneemishere.github.io/image-inpainting/">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="robots" content="index, follow">

  <meta property="og:title" content="Beyond CNNs: Encoded Context for Image Inpainting with LSTMs and Pixel CNNs">
  <meta property="og:description"
    content="Current image inpainting techniques are too heavy; this paper introduces a Row-wise Flat Pixel LSTM, a small hybrid model for the efficient and high-quality restoration of small images.">
  <meta property="og:url" content="https://taneemishere.github.io/image-inpainting/">
  <meta property="og:type" content="website">
  <meta property="og:image" content="https://taneemishere.github.io/image-inpainting/images/fig_10.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Beyond CNNs :: Image Inpainting">
  <meta name="twitter:image" content="https://taneemishere.github.io/image-inpainting/images/fig_10.png">

  <title>Beyond CNNs :: Image Inpainting</title>

  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "ScholarlyArticle",
      "name": "Beyond CNNs: Encoded Context for Image Inpainting with LSTMs and Pixel CNNs",
      "author": {
        "@type": "Person",
        "name": "Taneem Ullah Jan",
        "url": "https://taneemishere.github.io"
      },
      "url": "https://taneemishere.github.io/image-inpainting/",
      "description": "Current image inpainting techniques are too heavy; this paper introduces a Row-wise Flat Pixel LSTM, a small hybrid model for the efficient and high-quality restoration of small images.",
      "datePublished": "2024-04-29",
      "dateModified": "2025-09-25",
      "keywords": "Image Inpainting, GAN, Pixel CNN, LSTM, EfficientNet, ResNet, VGG, Autoencoder, Convolutional Neural Network, Generative Adversarial Network, Wasserstein GAN, Context Encoder",
      "inLanguage": "en"
    }
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./css/bulma.min.css">
  <link rel="stylesheet" href="./css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./css/bulma-slider.min.css">
  <link rel="stylesheet" href="./css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="./js/fontawesome.all.min.js"></script>
  <script src="./js/bulma-carousel.min.js"></script>
  <script src="./js/bulma-slider.min.js"></script>
  <script src="./js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><span style="color: red;">Beyond CNNs</span>: Encoded
              Context for Image Inpainting with LSTMs and Pixel CNNs</h1>
            <br>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://taneemishere.github.io" target="_blank">Taneem Ullah Jan</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="#">Ayesha Noor</a></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">University of Engineering and Technology Peshawar, Pakistan<br>
                <br>International Conference on Innovations in Computing Technologies and
                Information Sciences (ICTIS) 2024 <br> International Journal of Innovations in Science & Technology
                (<em>IJIST</em> ) {Vol. 6 No. 5 (2024): ICTIS Spl Issue}</span>
              <br>
              <span class="eql-cntrb"><small><br><sup>*</sup>First Author</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://journal.50sea.com/index.php/IJIST/article/view/783/1369" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://journal.50sea.com/index.php/IJIST/article/view/783" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>IJIST</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- architecture images -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <img src="./images/fig_3.png" alt="autoencoder model" />
            <h2 class="subtitle has-text-centered">
              Autoencoder architecture of our model.
            </h2>
          </div>
          <div class="item">
            <img src="./images/fig_10.png" alt="row flattened lstm model" />
            <h2 class="subtitle has-text-centered">
              Row Flattened LSTM Architecture.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Our paper presents some creative advancements in the image in-painting techniques for small, simple images
              for example from the CIFAR10 dataset. This study primarily targeted on improving the performance of the
              context encoders through the utilization of several major training methods on Generative Adversarial
              Networks (GANs). To achieve this, we upscaled the network Wasserstein GAN (WGAN) and compared the
              discriminators and encoders with the current state-of-the-art models, alongside standard Convolutional
              Neural Network (CNN) architectures. Side by side to this, we also explored methods of Latent Variable
              Models and developed several different models, namely Pixel CNN, Row Long Short Term Memory (LSTM), and
              Diagonal Bidirectional Long Short-Term Memory (BiLSTM). Moreover, we proposed a model based on the Pixel
              CNN architectures and developed a faster yet easy approach called Row-wise Flat Pixel LSTM. Our
              experiments demonstrate that the proposed models generate high-quality images on CIFAR10 while conforming
              the L2 loss and visual quality measurement.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">

          <div class="item">
            <img src="./images/fig_9.png" width="640" height="360" alt="Pixel CNN model result"
              style="display: block; margin: 0 auto;" />
            <h2 class="subtitle has-text-centered">
              On the left is ground truth example and on the right is a reconstructed image from the Pixel CNN model
            </h2>
          </div>

          <div class="item">
            <img src="./images/fig_11.png" width="640" height="360" alt="Row-wise Flat Pixel
            LSTM result" style="display: block; margin: 0 auto;" />
            <h2 class="subtitle has-text-centered">
              On the left is ground truth example and on the right is a reconstructed image from our Row-wise Flat Pixel
              LSTM
            </h2>
          </div>

          <div class="item">
            <img src="./images/fig_12.png" width="640" height="360" alt="Final result"
              style="display: block; margin: 0 auto;" />
            <h2 class="subtitle has-text-centered">
              Results from Out-of-sample example
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- paper description -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p>
              <strong>The Problem</strong> <br>
              In the field of computer vision, one challenge has consistently stood out due to its complexity and
              potential impact: image-inpainting. This technique is essential in restoring old photographs and repairing
              visual content, involves filling in missing or damaged portions of an image in a way that is seamless and
              unnoticeable. The goal is not just to cover the gaps but to understand the surrounding pixels' context and
              regenerate the lost part in harmony with the whole image. Traditional methods have halted in maintaining
              consistency in texture and color, often leaving noticeable mismatches that fail the alterations.
            </p>

            <strong>Innovating image restoration with deep learning</strong> <br>
            <p>Our work was done to advance this process with the help of deep learning, bringing a subtle understanding
              of images at a latent level. We also explored the area of Generative Adversarial Networks (GANs) and
              specifically adapted the network to a Wasserstein GAN (WGAN) framework. This adaptation, coupled with
              strategic training enhancements, reinforced our model's ability to handle the softness of image textures
              and colors. Also, our exploration extended into density-based methods, where we experimented with Pixel
              CNN and various LSTM configurations. A significant development was the introduction of a unique process we
              calledÂ Row-wise Flat Pixel LSTM, simplifying the traditionally complex architectures. This new method was
              not just about streamlining but also about enhancing the model's interpretability of the image data.
            </p>

            <strong>Future directions: beyond the frame!</strong>
            <p>While our results are promising, the field of innovation remains vast. Potential future explorations
              include expanding the model's capabilities to handle more complex and higher-resolution images and videos,
              which are essential for real-world applications. Furthermore, integrating more advanced GAN and Diffusion
              architectures or using unsupervised learning methods could unlock even more sophisticated inpainting
              techniques. Another promising area is exploring real-time inpainting for video content, a frontier not yet
              fully explored. This advancement could revolutionize film restoration, live broadcasting, and even video
              editing by providing tools to repair visual content on-the-fly.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- paper slides/poster -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Slides</h2>
        <iframe src="./slides.pdf" width="100%" height="550">
        </iframe>
      </div>
    </div>
  </section>

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTex</h2>
      <pre><code>@article{
        Jan_Noor_2024, 
        title={Beyond CNNs: Encoded Context for Image Inpainting with LSTMs and Pixel CNNs}, 
        volume={6}, 
        url={https://journal.50sea.com/index.php/IJIST/article/view/783},  
        number={5}, 
        journal={International Journal of Innovations in Science &amp; Technology}, 
        author={Jan, Taneem Ullah and Noor, Ayesha}, 
        year={2024}, 
        month={May}, 
        pages={165-179} 
      }
      </code></pre>
    </div>
  </section>

  <!-- footer -->
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content" style="text-align: center;">
            <p>
              Website template adapted from <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">here</a>.
            </p>
            <p> <a href="https://info.flagcounter.com/48VZ"><img
                  src="https://s01.flagcounter.com/mini/48VZ/bg_FFFFFF/txt_000000/border_CCCCCC/flags_0/" alt=""
                  border="0"></a> </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>